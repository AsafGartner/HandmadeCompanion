---
title: "Memory Barriers and Semaphores"
videoId: "W_szrzjYuvs"
markers:
	"0:00:35": "Recap/Review"
	"0:02:45": "Yesterday's TODOs"
	"0:04:00": "TODO 1: Ordering of writes"
	"0:06:33": "CompletePastWritesBeforeFutureWrites"
	"0:10:22": "Looking up memory fences"
	"0:12:10": "Venturing into Visual Studio's include folder"
	"0:13:20": "Putting in an actual CPU barrier"
	"0:14:32": "Where do we go from here?"
	"0:15:00": "Volatile introduction"
	"0:18:43": "TODO 2: Interlocked writes"
	"0:20:10": "Looking up InterlockedIncrement"
	"0:21:37": "TODO 3: Already taken care of by volatile"
	"0:21:54": "TODO 4: Ordering of reads"
	"0:23:24": "Checking our work"
	"0:24:20": "NB: Don't use all 16 hyperthreads!"
	"0:25:45": "Job completion"
	"0:28:40": "Waiting for all the threads to complete"
	"0:30:50": "Suspending and resuming threads"
	"0:34:51": "Semaphores"
	"0:37:28": "WaitForSingleObject for suspending"
	"0:40:35": "Creating the semaphore"
	"0:44:24": "Problem: Threads never wake up!"
	"0:46:50": "Waking up by releasing the semaphore"
	"0:53:27": "How we'll use the semaphore"
	"0:54:26": "Testing the semaphore"
	"0:56:50": "Work for tomorrow"
    "0:57:13": "Q&A"
	"0:58:24": "d7samurai Q: In the visual studio output window you can right click and deselect some of the stuff"
	"0:58:56": "cubercaleb Q: Why does this have to be so complicated??"
	"0:59:37": "rc1290 Q: Why did you put the memory barrier in a macro when it's platform-specific code?"
	"1:00:07": "robrobby Q: After the sleep, some threads still pushed several strings, leaving out some of the other threads"
	"1:00:56": "plain_flavored Q: Would Sleep(0) in your spin-lock help anything?"
	"1:02:35": "abnercoimbre Q: IMGUI has been trending. Sorry if you get asked this more than you like to."
	"1:03:07": "goodjerm Q: When you initially started this project, what were the first 5 things you coded and why?"
	"1:03:22": "ifingerbangedurcat Q: I missed most of tonight, what does the volatile keyword mean?"
	"1:03:47": "noxy_key Q: How do you plan to maintain cache line coherency between processors? Can physical CPUs share a cache line?"
	"1:05:03": "Blackboard: MESI and Cache Coherency"
	"1:14:15": "gasto5 Q: Was volatile added in C99?"
	"1:14:37": "abnercoimbre Q: Wait, so transactional memory wants to simplify concurrent programming by allowing a chunk of load/store instructions to execute in atomically. Have you messed with this?"
	"1:15:05": "Blackboard: Transactional Memory"
	"1:18:58": "abnercoimbre Q: Transactional memory is often advocated as an easier-to-use replacement for locks that avoids any possibility of a deadlock, so I wanted your thoughts."
	"1:21:04": "plainflavored Q: Why are we building a generic work distribution when the tiled renderer is designed to cleanly split up the work anyway?"
	"1:22:45": "Wrapping things up"
---

Semaphores:

A semaphore is essentially a number that the operating system keeps track of, that can be incremented and decremented.
When you wait for a semaphore, you're essentially telling the OS to let you know when the semaphore number becomes
greater than zero. Once it does, then the Wait() call will return and the thread can do something. Calling
ReleaseSemaphore(), maybe a little counterintuitively, increments the semaphore, allowing any threads waiting on it to
continue working. (Thus, it *releases* those threads to do work). It doesn't actually change the state of the semaphore
other than making the number go up. The semaphore number goes down when a thread has successfully Wait()ed for the
semaphore. In cases like the one demonstrated on stream, this usually means the semaphore number will go up/down really
fast and stick close to 0 as most of the time the threads are waiting for the semaphore to increment.

What this allows you to do is tell several threads at once that some work is ready without having to signal to each one
individually. As long as each one is waiting on the same semaphore object, they'll all know when there's more work to be
done.
